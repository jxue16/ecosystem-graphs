---

- type: model
  name: Guanaco
  organization: University of Washington
  description: Guanaco is a model family trained with QLORA, an efficient finetuning approach that reduces memory usage enough to finetune a 65B parameter model on a single 48GB GPU while preserving full 16-bit finetuning task performance.
  created_date: 2023-05-23
  url: https://arxiv.org/pdf/2305.14314v1.pdf
  model_card: ‘’
  modality: natural language text
  analysis: Reports results on the Vicuna benchmark and compares performance level and time expenditure with ChatGPT
  size: 33B parameters 
  dependencies: [Github]
  training_emissions: ‘’
  training_time: ‘’
  training_hardware: 24 GB GPUs
  quality_control: ‘’
  access: open
  license: MIT
  intended_uses: ‘’
  prohibited_uses: ‘’
  monitoring: ‘’
  feedback: ‘’
